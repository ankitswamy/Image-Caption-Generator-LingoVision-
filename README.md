# Image-Caption-Generator-LingoVision-
Image Caption Generator with Translation &amp; Audio Output This repository contains an encoder-decoder deep learning model for generating descriptive captions from images. Built with TensorFlow/Keras, it leverages InceptionV3 for image feature extraction and an LSTM-based decoder for text generation.

The aim of this project was to develop an intelligent image captioning system capable of generating meaningful and grammatically correct textual descriptions for a given image. This system integrates computer vision and natural language processing techniques to bridge the gap between visual understanding and language generation. To achieve this within limited computational resources and time, a basic encoder-decoder framework was implemented using a pretrained InceptionV3 model as the encoder for feature extraction and an LSTM-based decoder to generate captions. The system was further enhanced by translating the generated captions into other languages using Google Translate API, followed by converting them into speech using gTTS, enabling multimodal output in both text and audio formats.

The methodology involved extracting image features using InceptionV3, followed by sequence generation using greedy decoding to form captions. BLEU score evaluation was employed to assess the quality of the generated captions. The model demonstrated satisfactory performance in generating coherent and relevant descriptions for various test images. This foundational implementation lays the groundwork for future enhancement, where advanced techniques such as ResNet-101 with CBAM for feature refinement, Faster R-CNN for object detection, and multimodal attention mechanisms in the decoder will be integrated to improve caption quality, accuracy, and context-awareness.
